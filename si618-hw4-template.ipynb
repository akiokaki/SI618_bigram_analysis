{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 618 WN 2018 - Homework 4: Using the Spark RDD API to analyze bigrams in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">NOTE: If your PySpark installation isn't working please plan to attend office hours or set up an appointment with Kai (Mac or Windows) or Dr. Teplovs (Mac) as soon as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. To gain familiarity with PySpark\n",
    "2. To learn the basics of the Spark RDD API\n",
    "3. To get experience finding and downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please fill in...\n",
    "### * Your name: Akio Kakishima\n",
    "### * People you worked with:  I worked by myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions:\n",
    "Please turn in this Jupyter notebook file (in both .ipynb and .html formats) **and the text file that contains the text you analyzed** via Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project is designed to give you a basic familiarity with the Apache Spark RDD API.\n",
    "\n",
    "**Your first task** is to run the next code cell in this notebook, as is, (without modification) and confirm that everything is working.\n",
    "\n",
    "Your second and main task is to modify the word_count_v2.ipynb file in Lab 4 to create a si618-hw4-YOUR_UNIQNAME, which counts the number of bigrams within the corpus. At the conclusion of its execution, it should output three pieces of information: \n",
    "1. the total number of bigrams\n",
    "2. the 20 most common bigrams\n",
    "3. the minimum number of bigrams required to add up to 10% of all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11),\n",
       " ('was', 10),\n",
       " ('of', 10),\n",
       " ('it', 9),\n",
       " ('we', 4),\n",
       " ('us', 2),\n",
       " ('age', 2),\n",
       " ('season', 2),\n",
       " ('epoch', 2),\n",
       " ('before', 2)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "totc = [\"It was the best of times\",\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom\",\n",
    "    \"it was the age of foolishness\",\n",
    "    \"it was the epoch of belief\",\n",
    "    \"it was the epoch of incredulity\",\n",
    "    \"it was the season of Light\",\n",
    "    \"it was the season of Darkness\",\n",
    "    \"it was the spring of hope\",\n",
    "    \"it was the winter of despair\",\n",
    "    \"we had everything before us\",\n",
    "    \"we had nothing before us\",\n",
    "    \"we were all going direct to Heaven\",\n",
    "    \"we were all going direct the other way\"]\n",
    "\n",
    "WORD_RE = re.compile(r\"\\b[\\w']+\\b\")\n",
    "\n",
    "input_text = sc.parallelize(totc)\n",
    "word_counts_sorted = input_text.flatMap(lambda line: WORD_RE.findall(line)) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda accumulator, value: accumulator + value) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "top10_sorted = word_counts_sorted.take(10)\n",
    "top10_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Analysis\n",
    "Bigrams are pairs of continguous words.  For example, consider the text \"The quick brown fox\".  The bigrams in that text are: (The, quick), (quick, brown), (brown,fox).\n",
    "\n",
    "You should be able to use the above code as a starting point, with the main difference being that you will be extracting pairs of words rather than single words.\n",
    "\n",
    "Here are the steps you need to include:\n",
    "\n",
    "1. Find and download a text file from the Gutenberg Project.  Please select a book written in English and download the plain text version (i.e. the .txt file).  \n",
    "1. Normalize the text by converting it to lowercase.  \n",
    "1. Extract all bigrams from the text.  Don’t try to get fancy: just take all the pairs of words from each line of your text file\n",
    "1. Perform a mapping to yield a count of one for each bigram (e.g. ((\"the\", \"quick\"), 1)). \n",
    "1. Reduce this list of bigrams with count of one to counts of bigrams (e.g. ((\"the\", \"quick\"), 312)).\n",
    "1. Sort to determine the most frequent and total number of bigrams. \n",
    "1. Report the total number of bigrams, the top 20 most common bigrams and the minimum number of bigrams required to add up to 10% of all bigrams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Find and download a text file\n",
    "Go to Project Gutenberg (http://www.gutenberg.org) and find a book written in English that you think sounds interesting. \n",
    "Download the plain text (.txt) version of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Load the text file into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime_punishment_text = \"crime_punishment.txt\"\n",
    "crime_punishment = sc.textFile(crime_punishment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2 and 3: Normalize text and extract bigrams\n",
    "Note: there are many ways to accomplish this.  We recommend you create\n",
    "a function that both creates bigrams and normalizes text.  The following\n",
    "template code assumes this is the approach you are planning to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'project'),\n",
       " ('project', 'gutenberg'),\n",
       " ('gutenberg', 'ebook'),\n",
       " ('ebook', 'of'),\n",
       " ('of', 'crime'),\n",
       " ('crime', 'and'),\n",
       " ('and', 'punishment'),\n",
       " ('punishment', 'by'),\n",
       " ('by', 'fyodor'),\n",
       " ('fyodor', 'dostoevsky')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def some_function(line_taken):\n",
    "    list_words = []\n",
    "    list_words = WORD_RE.findall(line_taken.lower())\n",
    "\n",
    "    counter = 0\n",
    "    list_bigrams = []\n",
    "    for idx, each_word in enumerate(list_words):\n",
    "        if idx == 0:\n",
    "            pass\n",
    "        else:\n",
    "            bigram = (list_words[idx-1],each_word)   # THIS HAS TO BE A TUPLE\n",
    "            list_bigrams.append(bigram)\n",
    "    return list_bigrams\n",
    "    \n",
    "bigrams1 = crime_punishment.flatMap(lambda line: some_function(line))\n",
    "# Debug: make sure it's a list of bigrams\n",
    "temp_output = sc.parallelize(bigrams1.take(10))\n",
    "temp_output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Map the bigrams to key-value pairs where the key is the bigram and the value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'project'), 1),\n",
       " (('project', 'gutenberg'), 1),\n",
       " (('gutenberg', 'ebook'), 1),\n",
       " (('ebook', 'of'), 1),\n",
       " (('of', 'crime'), 1),\n",
       " (('crime', 'and'), 1),\n",
       " (('and', 'punishment'), 1),\n",
       " (('punishment', 'by'), 1),\n",
       " (('by', 'fyodor'), 1),\n",
       " (('fyodor', 'dostoevsky'), 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams2 = bigrams1.map(lambda each_bigram: (each_bigram, 1))\n",
    "\n",
    "# Debug: make sure it's a list of bigrams\n",
    "temp_output = sc.parallelize(bigrams2.take(10))\n",
    "temp_output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Reduce the (word,1) key-value pairs to give you counts of each bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams3 = bigrams2.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sort the resulting RDD by value in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigrams_sorted = bigrams3.sortBy(lambda x: x[1], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Report the required values\n",
    "1. the total number of bigrams\n",
    "2. the 20 most common bigrams\n",
    "3. the minimum number of bigrams required to add up to 10% of all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bigrams: 77794\n"
     ]
    }
   ],
   "source": [
    "#1. the total number of bigrams\n",
    "total_bigrams = bigrams_sorted.count()\n",
    "print(\"Total number of bigrams: {}\".format(total_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'the'), 757),\n",
       " (('of', 'the'), 585),\n",
       " (('he', 'was'), 481),\n",
       " (('to', 'the'), 481),\n",
       " (('don', 't'), 464),\n",
       " (('he', 'had'), 460),\n",
       " (('on', 'the'), 458),\n",
       " (('it', 's'), 451),\n",
       " (('i', 'am'), 441),\n",
       " (('at', 'the'), 440),\n",
       " (('it', 'was'), 399),\n",
       " (('that', 's'), 355),\n",
       " (('that', 'he'), 323),\n",
       " (('you', 'are'), 310),\n",
       " (('to', 'be'), 301),\n",
       " (('in', 'a'), 301),\n",
       " (('do', 'you'), 282),\n",
       " (('with', 'a'), 253),\n",
       " (('did', 'not'), 248),\n",
       " (('for', 'the'), 241)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. the 20 most common bigrams\n",
    "top20 = sc.parallelize(bigrams_sorted.take(20))\n",
    "top20.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of bigrams required to add up to 10% of all bigrams: 19\n"
     ]
    }
   ],
   "source": [
    "#3. the minimum number of bigrams required to add up to 10% of all bigrams\n",
    "bigram_list = sc.parallelize(bigrams_sorted.take(10000))\n",
    "percentage = 0.1\n",
    "accum_total = 0\n",
    "i_list = []\n",
    "for i in bigram_list.collect():\n",
    "    if accum_total < (total_bigrams*percentage):\n",
    "        accum_total += i[1]\n",
    "        i_list.append(i)\n",
    "    else: pass\n",
    "print(\"Minimum number of bigrams required to add up to 10% of all bigrams: {}\".format(len(i_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above and Beyond\n",
    "Crime and Punishment is the story of the protagonist, Raskolnikov (also known as Rodia). It tells the story of his anguish, guilt and glee of murdering a pawnbroker. People play an important role in his well being in the book. For example, the police officer in charge of investigating the death will cause negative impact on Rodia, while the thoughts of Rodia's sister will mostly bring him joy. \n",
    "\n",
    "In this kernel, I will analyze the frequency of a Names that tells a snapshot of how often each character is addresssed. I will filter out some basic words that may not point to character names that will still be caught by the regex. Pronouns are kept in, as they may be important in showing what sort of people are addressed (whether it is a man or woman, or self, other...)\n",
    "This deserves extra credit because I will be reanalyzing the text, this time with a single word, as opposed to a bigram. \n",
    "\n",
    "To no surprise, Raskolnikov pops up as the top. Interestingly, Svidrigaïlov shows up as second-- he is a character known to symbolize human desire, and generally bad vibes. Svidrigaïlov is followed by Rodia's sister, Dunia.\n",
    "\n",
    "\n",
    "Changes\n",
    "\n",
    "-Regex now filters through for words that are capitalized\n",
    "\n",
    "-Filter words that are not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 267),\n",
       " ('Raskolnikov', 208),\n",
       " ('She', 57),\n",
       " ('Svidrigaïlov', 50),\n",
       " ('Dounia', 42),\n",
       " ('Ivanovna', 41),\n",
       " ('Razumihin', 38),\n",
       " ('Petrovitch', 35),\n",
       " ('It', 33),\n",
       " ('Katerina', 33),\n",
       " ('Porfiry', 28),\n",
       " ('His', 28),\n",
       " ('Pyotr', 25),\n",
       " ('You', 24),\n",
       " ('Zametov', 18),\n",
       " ('Pulcheria', 16),\n",
       " ('Gutenberg', 16),\n",
       " ('Lebeziatnikov', 15),\n",
       " ('Marmeladov', 14),\n",
       " ('Romanovna', 13)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above_re = re.compile(r\"\\b^[A-Z][\\w]+\\b\")\n",
    "#junk_list = [\"And\", \"But\", \"You\",\"The\",\"What\",\"Why\",\"How\",\"She\",\"That\"]\n",
    "word_counts_sorted = crime_punishment.flatMap(lambda line: above_re.findall(line)) \\\n",
    "                        .map(lambda word: (word, 1)) \\\n",
    "                        .reduceByKey(lambda a, b: a + b) \\\n",
    "                        .sortBy(lambda x: x[1], ascending = False) \\\n",
    "                        .filter(lambda x: \"And\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"But\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"The\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"A\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"When\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"Where\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"Who\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"How\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"That\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"This\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"So\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"In\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"For\" not in x[0]) \\\n",
    "                        .filter(lambda x: \"At\" not in x[0])                 \n",
    "#for i in junk_list:\n",
    "#    word_counts_sorted.filter(lambda x: i not in x[0])\n",
    "top20_sorted = sc.parallelize(word_counts_sorted.take(20))\n",
    "top20_sorted.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
